\chapter{Architecture}
\label{chap:architecture}

%TODO
TODO: rework everything when you are done with the related work chapter.
See implementation stuff on garbage collection for deleted events, handling of conflicts, etc.

The following section will define the data model each peer keeps of its data and specify the API for updating it between peers.
First we will define and explain the model each peer keeps of the stored data.
Next the messages used to facilitate the update of the model between clients will be discussed.
Finally we will take a look at the advanced features and how they will be built on top of the previous work.

\section{Peer Data Model}

%TODO: ignore files / dirs? This should be implemented...

% why again json
Since we intend to use JSON for communication via Tox, it seems prudent to define the file model of a peer's data in the same way.
This will allow easy application of updates without having to explicitly translate between two different views of the same data.

An important feature that the model must have is that it works independent of the file types.
Therefore there are only two assumptions we will make of the structure of the directory that a peer will work on: namely that it contains files sorted in directories.
Out of this we can immediately synthesize our two main components we will require: a file model and a directory model.
Since a peer is intended to have directory as the root node from which to run, the core element will always be a directory.
An example of the proposed model structure can be seen in~\ref{list:model}.

\begin{figure}[htp]
\begin{modellist}
\item Root Directory
    \begin{modellist}
        \item File
        \item Sub Directory
            \begin{modellist}
                \item File
                \item File
            \end{modellist}
        \item File
        \item File
    \end{modellist}
\end{modellist}
\caption[Data Model Example Structure]{An example of how a data model of a directory is structured.}
\label{list:model}
\end{figure}

The most important aspect is of course what information is stored within the model and its objects.
Each file is considered a binary blob and not modified by Tinzenite in any way to preserve data integrity.
Therefore additional information that Tinzenite requires to function will need to be part of the model.

Each object in the model will for identification purposes be specified by a hash.
This hash will be a unique randomly generated hash.
This hash allows us to decouple the name of the object from its model representation, effectively serving the same function as node identification numbers when stored on a disk.
This is required so that we can differentiate a renamed file from a removed file and update a model accordingly.
Furthermore each model object will contain a path variable that specifies the complete path of the object in the directory tree.
This has the purpose of allowing the placement of all files in the correct locations on disk.
Notably, unlike on the user's disk, the names contained in the path are the identifying hashes.

Apart from the above attributes Tinzenite must also track versions of files to allow detection of when files have been updated.
We will use a vector clock\footnote{TODO: find and link paper?} to implement this.
The vector of peers with the associated versions can also be used to detect collisions.
Some further attributes will also be specified, but as they differ between file and directory model, these will be further discussed in the corresponding sections below.

It is important to note that the model will not be used to store peer reliant information.
This includes for example where the directory is placed on the peer's file system.
Such information must be stored separately by the peer and applied when working with the data model, for example when determining what the full path on the file system will be for a file that is to be written.
Some properties are also not suited to be transferred between peers.
This includes file system or operating system dependent properties such as usage rights, ownership, or flags.
%TODO look into this. A security argument might be to simply strip all extra information on transit but leave it untouched on client side, even over file updates. This would mean that the user only has to setup a dir once with rights etc as he likes it before he can use it with Tinzenite.

\subsection{Directory Model}
\label{sec:dir_model}

\begin{figure}[htp]
    \begin{lstlisting}[language=json,firstnumber=0]
    {
        "identification":"subdir_random_hash",
        "path":"root_hash/subdir_random_hash",
        "name":"subdir",
        "shadow":"false",
        "version":{
            "peer_name", "version",
            "peer_name", "version",
            ...
        },
        "objects":[
            ...
        ]
    }
    \end{lstlisting}
\caption[Directory JSON Model]{An example of a directory JSON object. Note that for brevity no files or sub directories are shown in the \textit{"objects"} array.}
\label{json:directory_model}
\end{figure}

Figure~\ref{json:directory_model} shows an example of the proposed JSON structure for representing a directory.
The \textit{"identification"} attribute is a random generated hash that uniquely identifies the directory.
The \textit{"path"} attribute stores the concatenated relative full path from the peers root directory, notably replacing the real names of the directories with the corresponding identification attribute values.
For trusted peers that store an unencrypted version of the directory the clear text \textit{"name"} is also stored here as an attribute.
The \textit{"shadow"} flag is used to signal whether the contents of the directory are to be fetched or not.
Note that this flag is not indicative of whether any nested objects are also shadow objects\footnote{This means that the flag is only read on receiving update messages: if no different policy for the updated object has been set the directory flag is used to determine whether to fetch the actual file or not. Directories are always created though.}.
To differentiate between updates we require a \textit{"version"} attribute which represents a vector clock of peers and their last known version.
Finally an \textit{"objects"} array is where the corresponding sub directories or files are placed.

\subsection{File Model}
\label{sec:file_model}

\begin{figure}[htp]
    \begin{lstlisting}[language=json,firstnumber=0]
    {
        "identification":"file_random_hash",
        "path":"root_hash/subdir_random_hash/file_random_hash",
        "name":"file",
        "version":{
            "peer_name", "version",
            "peer_name", "version",
            ...
        },
        "content":"content_hash",
        "shadow":"false"
    }
    \end{lstlisting}
\caption[File JSON Model]{An example of a file JSON object.}
\label{json:file_model}
\end{figure}

Figure~\ref{json:file_model} shows an example of the proposed JSON structure for representing a file object.
The \textit{"identification"} attribute is a random generated hash that uniquely identifies the file.
The \textit{"path"} attribute stores the concatenated relative full path from the peers root directory, notably replacing the real names of the directories with the corresponding identification hashes, and finally the file name with its hash too.
For trusted peers that store an unencrypted version of the file the clear text \textit{"name"} is also stored here as an attribute.
To differentiate between updates we require a \textit{"version"} attribute which represents a vector clock of peers and their last known version.
Important for detecting file changes is the \textit{"content"} attribute which stores a hash of the file's binary blob.
Finally the \textit{"shadow"} flag is used to notify a peer whether the file is locally accessible or must first be fetched from other peers.
On sending updates it is also indicative to the receiving peer whether the file can be fetched from the sending peer: if the value is true than the peer does not have a copy of the file, only its model object.

\subsection{Peer Futher Data}

Apart from the model of the directory for reconciliation, each peer requires further information to work.
The following short section describes the further information that strictly speaking does not belong to the directory model, but is still required for Tinzenite to work correctly.

\begin{figure}[htp]
    \begin{lstlisting}[language=json,firstnumber=0]
    {
        "user":"hash_of_username.salt",
        "directory_id":"random_hash",
        "key":"encrypted_data.salt",
        "active_peers":[
            "peer_tox_id",
            ...
        ],
        "removed_peers":[
            "peer_tox_id",
            ...
        ]
    }
    \end{lstlisting}
\caption[Authentication JSON Object]{Shows the proposed contents of the authentication block. Apart from identifying information and the encrypted data encryption key note the peer lists â€“ one for active peers and one for removed peers.}
\label{json:auth_object}
\end{figure}

Figure~\ref{json:auth_object} shows the management block that a peer keeps.
The value of \textit{"user"} allows the allocation of a directory to a user name.
This is important for example for the support of encrypted third party peers: they can attach accounts to the provided user name for controlling server side access.
Therefore we also need a way to distinguish multiple synchronized directories from each other: this is the random unique hash stored in \textit{"directory\_id"}.
Again, this can be used by third party service providers to differentiate the amount of directories that a user can store with them.
The symmetric key for encrypting and decrypting data for encrypted peers is stored in \textit{"key"}, encrypted in turn by a to be defined scheme that decrypts it upon entry of the correct password by the user.

Note that changing any of the values in the authentication block is not supported initially.
To change the password or the user name hash, the Tinzenite peer must be removed and added anew.
While cumbersome it offers the highest level of protection against possible security risks for now.

Finally we also use the authentication block to store and synchronize the peer list of connected peers of a synchronized directory.
As peers can be removed we need to keep a list of removed peers too.
Peers that have been removed, thus placing their Tox identification within the removed list, can not be reactivated.
The peer must be recreated and added as if it were a new peer.
Since we do not expect a large number of peers to be removed and added over the lifetime of a Tinzenite directory, this list is never trimmed.
A peer can detect a new peer by noticing its existence upon synchronization with another peer.
If the new peer Tox identification is not in the removed list it is simply added to the active list.
This ensures that peers that have been removed will never be added back to the active list which might pose a security risk\footnote{TODO: do something similar for removing files. Note that we WILL have to trim that list though at some point in time, but since we know which peers are active, this should be doable.}.

\section{Core Update API}
\label{sec:Core Update API}

This section describes the application interface that will be used to synchronize two models on separate peers where the models might diverge.
To understand how this works, it is important to understand how Tinzenite is structured.

Every peer views all connected peers as separate connections.
A swarm behavior only comes to pass because of the independent actions of every peer, not through a combined communication between multiple peers.
This primarily makes it relatively easy to implement a Tinzenite peer, as the communication state is never between multiple peers.
Therefore a single peer has a base state of no connected other peers.

\subsection{Connection Management}
\label{sec:conn_management}

In this section we will discuss how Tinzenite connects to other peers.
As we distinguish between trusted and encrypted peers we must determine how the data we will send is to be modified accordingly.
If the initiating peer is already encrypted, the connection by default will be considered anonymous.
If not the peer must query the other peer.
Since we can not trust the other peer to give an honest answer this query must be cryptographically secure.
Attackers that illegitimately wants to connect to a trusted peer thus can not establish a clear connection without the required keys, which they should not be in possession of\footnote{Tinzenite is designed to avoid revealing keys to outsiders. However outside attacks are beyond its control such as for example key loggers.}.
Only if the challenge is successfully replied to is the other peer considered trusted and the following communication not anonymous.
If the challenge is ignored the other peer is considered as an encrypted peer and the following communication adapted to fit.
Should the challenge be incorrectly answered we can consider possibly warning the user or even blacklisting the peer.

Once the connection type has been determined it will be applied to all following communication.
In general this means that compared to a trusted communication identifying information and file contents will be encrypted.
The peer list will be synchronized but the encryption keys not.
The model exchange and all update messages will be partially encrypted too to preserve data security.

The next and final step to a ready and idle connection state is the exchange of models.
Once this is done both peers can compare the received models with their local ones and determine which files they need to update from the other peer.
These messages will then be sent asynchronously over the now established and typified communication channel accordingly.

\begin{figure}[htp]
\centering
    \includegraphics[width=\textwidth]{graph/sm_peer_communications}
\caption[Connection State Diagram]{This diagram shows how peers handle the establishment of a connection to a known peer.}
\label{graph:connection_states}
\end{figure}

Figure~\ref{graph:connection_states} shows an informal state diagram of how a peer reaches the idle state where the connection is ready and set up.
The switch from the disconnected state to the connected state is signaled by the underlying Tox connection, meaning it happens when the other peer is online and visible via the Tox channel.
Once connected and if the peer itself is a trusted peer, a challenge is sent which contains a general identifier and an encrypted nonce.
The encrypted nonce is important to check whether the other peer is encrypted or trusted.

Note that a few points must be considered that can not be shown in the diagram.
Since Tinzenite is designed as a peer to peer network, no client server structure exists.
This poses the challenge of who begins the construction of a connection, especially as we can not distinguish between peers that have been online for a while already and peers that just activated when the base peer starts up.
This is due to peer to peer architecture of Tox: not all peers will respond to queries within the same time frame.
Peers further away from a network perspective will seem to be online only after peers that are nearer.
We solve this issue by having the peers use a random back off time whenever a request expects an answer\footnote{For example the challenge message for establishing a trusted connection.}.

Since we can not trust the other side of the Tox channel to be a Tinzenite peer, the peer initiating a connection will not advance beyond a step where it expects an answer.
This means that for example an attacker would either receive only a challenge (to which he can't respond without knowing the keys) or the anonymized model and peer list\footnote{The peer list is the more problematic of the two as it can be used to determine the size of the user's Tinzenite peer network. However to allow encrypted peers to facilitate file transfer between two mutually exclusively online peers, it must know this information. This is mitigated by the fact that Tox IDs are hard to guess and not shared beyond the Tinzenite peer network. TODO: move this to the security section / chapter / whatever?} (which doesn't offer up any useful information as per specifications).

\subsection{New Connection Establishment}

The previous section described how Tinzenite connects to known peers.
This section therefore discusses how Tinzenite connects to new peers, meaning peers that the user adds to the network.
Notably this is a manual process where the user is used as a secondary channel to prevent man in the middle attacks.
Since a typical Tinzenite network might have many peers, it is not desirable to have to authenticate a new peer manually with every other peer.
We therefore propose that the user only need authenticate the new peer with one existing peer.
From there the authentication is synchronized to the other existing peers much like any other file or property update.
This bootstrapping process allows a user friendly setup of peers.
Should the same peer have been added multiple times at different existing peers Tinzenite will detect this and merge the authentication together.

\begin{figure}[htp]
\centering
    \includegraphics[width=10cm]{diagram/sequence_new_connect}
\caption[New Connection Sequence Diagram]{This diagram shows the interaction required to connect a new peer to the existing Tinzenite network.}
\label{diagram:new_connection}
\end{figure}

Figure~\ref{diagram:new_connection} shows how a user interacts with a Tinzenite peer and the Tinzenite network (consisting of 1 to n other existing peers) to connect a new peer to the network for the first time.
As prose: the users simply start the new peer client for the first time.
Then they can point it at the directory location, change settings, and check that another peer is visible.
To establish an initial connection a Tox ID of an available Tinzenite peer is required.
The users can then command the new peer to connect to this available peer by entering the Tox ID.
To ensure that no man in the middle attack can happen the users will now have to confirm the connection on the available peer.
This means allowing the connection there and ideally ensuring that the seen Tox ID is identical with the new peer's Tox ID.
Once the connection has been confirmed on the other peer a channel is opened.

The peer then receives the authentication block to set up the basic information it requires.
Notably this also allows the user to define the new peer as a trusted peer by entering the password that unlocks the encryption key used for the stored data.
Access to the key also allows the peer to respond to other peers as a trusted peer.
The new peer address and its information will now be synchronized throughout the already connected peers once they are available, removing the need to add the new peer manually to each and every one.

Now the channel has been established and the peers can begin communicating as shown in figure~\ref{graph:connection_states}.
Notably though the starting state is the \textit{"Connected"} state as the connection has already been established on the Tox layer.
Depending on whether the user has specified the new peer as a trusted peer by entering the password or as an encrypted peer by skipping that step the peer will be defined accordingly.
The directories will begin synchronization once the challenge and model exchange have happened.

Removing peers will work much in the same way from a system perspective.
A user can note a peer to be removed, resulting in it being removed immediately from the peer where the action was initiated.
It will then transit through online peers and result in a complete removal once all peers have been online.
An interesting aspect to how to handle removal from the perspective of the to be removed peer is what to do with the data on the peer.
For a trusted peer we might offer a choice whether to remove the data or simply disconnect it from the Tinzenite network.
On the other hand encrypted peers can simply remove the data immediately as they can't access it anyway.

\subsection{Model Management}

This section will describe how a peer receives and sends messages via the Tox channel.
Since we will be required to distinguish between encrypted and trusted connections we will start off with the trusted one and then briefly highlight the differences required for an encrypted connection.

Once the setup has been completed, as seen in section~\ref{sec:conn_management}, the models must be updated if they do not match.
This happens normally when files and directories have changed through user or system interaction.

\subsubsection{Fetch File Message}

\begin{figure}[htp]
    \begin{lstlisting}[language=json,firstnumber=0]
    {
        "operation":"fetch",
        "identification":"object_identification_hash",
        "TODO":"ADD LIBRSYNC STUFF HERE",
        "delta":"true/false"
    }
    \end{lstlisting}
\caption[Fetch Object Message]{The message sent to initiate a file transfer.}
\label{json:fetch_file}
\end{figure}

Figure~\ref{json:fetch_file} shows how a message to initiate a file transfer is structured.
Note that apart from the identification we require no further information â€“ the model has been independently updated.
Now the peers need only get the binary files.
Upon receiving the message the other peer starts a Tox file transfer which the initiating peer knows to accept as it just requested it.

TODO: add capability for delta updates... if possible. :P
Delta is difficult because we must effectively track parts of files instead of complete files... how to do this?
use librsync! <-- Really good and easy to use, will save a ton of work!

TODO: Also remember that multiple file transfers might be a problem.
Do we limit the connection to one transfer at a time or can we fetch everything at once?

\subsubsection{Update Object Message}
\label{subs:Update Object Message}

\begin{figure}[htp]
    \begin{lstlisting}[language=json,firstnumber=0]
    {
        "operation":"create",
        "object":{
            ...
        }
    }
    \end{lstlisting}
\caption[Update Object Message]{The message broadcast by a peer to connected peers to notify that an update has happened. Operation can be any value of either create, remove, or modify.}
\label{json:update_object}
\end{figure}

Figure~\ref{json:update_object} is a message for broadcasting a model update to other connected peers.
The update message serves to keep both models in sync even after the initial synchronization at connection establishment.
Upon receiving an update message the peer will most likely immediately respond with a fetch message to retrieve the updated file.
If a peer receives a message for an operation that it has already applied, the message is silently discarded.
This does not disturb the propagation of updates since the peer will have already sent the update to the other connected peers if it has the update already.
In this way we ensure that every message propagates as far as possible without consuming unnecessary bandwidth\footnote{Peers are of course free to implement message sending in any way since the update messages are not critical for Tinzenite to work. Peers can simply only send update messages to a limited number of peers, down to none if bandwidth is scarce for example in the case of a mobile device.}.

\section{Object Operations}
\label{sec:Object Operations}

In this section we will discuss the object operations that Tinzenite requires to work.
Then we will specify the actual architecture of the messages we require for them.

\subsection{Operations}
\label{sub:Operations}

Tinzenite relies on the most basic file operations for manipulating both the model and the actual file directory.
Therefore we require only the following three operations for the basic case to work:

\begin{description}[leftmargin=5em,style=nextline,noitemsep,nolistsep]
    \item[Create]
        Files that have been created will be added to the model at the correct location and their attributes calculated, if not given.
        Tinzenite then checks whether it needs to fetch the file if it wasn't created in its own directory.
        Created files are detected by simply noticing files that do not exist in the model yet and are not listed as deleted.
    \item[Remove]
        File removing is one of the most complex cases in Tinzenite due to the insert delete ambiguity (see section~\ref{sub:Perspectives on Optimistically Replicated, Peer-to-Peer Filing}).
        We solve this by storing the models of deleted files until the delete update has propagated to all currently known peers.
        Only then is the model discarded also.
        For this to work Tinzenite must always ensure that files that exist but are listed as deleted are not added back to the model as a new file by continuously checking the deletion list.
    \item[Modify]
        Modification is either when the model does not match the file anymore, in which case a new file must be fetched, or the content on the local file has changed.
        This is detected via the content hash.
        In that case the model is updated to match the file again.
\end{description}

Of course those three operations are not the only operations that a user can do on directories or files.
However they are all the operations that Tinzenite requires so that it can offer full functionality.

Renaming objects can be handled by simply considering the old file to have been deleted and replacing it with the new file.
This new file will just so happen to share the same content hash.
Moving objects are handled much in the same way: by removing the file at its old location and creating it anew at the new location.

\begin{figure}[htp]
\centering
    \includegraphics[width=4cm]{graph/sm_object_lifecycle}
\caption[Object State Diagram]{The life cycle of every object, file and directory, in Tinzenite with the allowed operations.}
\label{diagram:object_operations}
\end{figure}

Therefore the life cycle of an object is very simple, as can be seen in figure~\ref{diagram:object_operations}.
Objects can only come into existence via the create operation.
Once an object it is only possible to either modify the existing object or remove it.
If an object has been removed it will stay removed unless the user creates a new clone of it.

\subsection{Specifications}
\label{sub:Specifications}

In this section we show the actual definition of the specification and expand on how Tinzenite peers react to them in detail.
This specification makes up the core functionality of how the data synchronization happens within Tinzenite.
Note that only trusted peers can initiate object operations: encrypted peers only react to messages and propagate them, never do messages originate from an encrypted peer.
This is simply the case due to the fact that encrypted peers store the data in a non accessible format, therefore making operating on the data rather pointless.

\subsubsection{Create}
\label{subs:Create}

The creation of an object can be trivially detected in Tinzenite.
Basically there are two cases where an object must be created: the first is local manual creation which implies that the peer is the origin of the file; the second is creation via receiving an creation message from another peer.

For the first case, if the object does not have a representation within the model and is not listed as deleted, it triggers the creation case.
Tinzenite creates the correct object representation and inserts it into the correct place in the model.
In the second case the peer will queue a fetch operation for the required file\footnote{Note that in the case that it receives an update with a set \textit{"shadow"} flag it will not try fetching the file from said peer as it doesn't have a copy of the file. As a special case files within shadowed directories will also never be fetched.}.
Only when the file has been received completely is it placed at the correct location and the model updated.

After the local model has been updated and everything is in order, the peer will send an update message to all connected peers.
Figure~\ref{json:update_object} shows how such a message might look â€“ note that the operation is labeled as \textit{"create"} accordingly.
It is then up to all receiving peers to update their models and fetch the file, then propagate the update accordingly.

\subsubsection{Modify}
\label{subs:Modify}

The modification of an object is not as trivially detected as an object creation â€“ for the specifics of update detection see the following section~\ref{sec:Update Detection and Reconciliation}.
Here we will only concern ourself with what happens once a modification has been detected.
Again we have two cases for triggering this case: a local file has been modified or the peer has received an update message.

If the modification is detected locally first we update the model to match and then initiate an update message.
The message is of the format as shown in figure~\ref{json:update_object} with the operation now reading \textit{"modify"} instead.
If, on the other hand, we receive such a message, we queue the fetching of the updated file.
Upon receiving the changes we apply them and finally propagate the update to the other connected peers.

\subsubsection{Remove}
\label{subs:Remove}

Finally the deletion of an object is the hardest case within Tinzenite because of the insert delete ambiguity as discussed in section~\ref{sub:Perspectives on Optimistically Replicated, Peer-to-Peer Filing}.
It is non trivial to ensure that an object deletion has been received by all required peers so that it truly and finally can be removed from the system.
A simple solution would be a list of all ever deleted objects, but unlike removed peers this list promises to grow quickly on often used synchronized directories.
Therefore we require a sort of garbage collection so that we can trim the list from deleted files that have been applied to all known peers.

For the base case we will look at how Tinzenite deletes an object that has been removed locally first, then discuss how the update propagates and what the other peers are required to do to ensure a safe and complete deletion.
If a deletion happens the object model is moved to a special list outside of the object model and the actual object removed from the disc.
Then the peer copies the list of currently known peers and places it with the object model.
Finally it creates a second list of peers in which it places itself.
This second list contains all the peers that have acknowledged receiving the deletion update.

This deletion update is then propagated to the other peers.
Each peer receives the update, removes the object and object model, enters itself into the second list, and propagates the update.
Once the last peer enters itself into the acknowledgement list, thus making it equal to the first list of peers, it propagates the update one final time and can then delete the object model from the deletion space.
The completed deletion update then traverses the peers backwards, each in turn deleting the update from its list and propagating it a last time.
Thus when the last peer to receive the full deletion update removes the model, we can be assured that all relevant peers have removed the object.

It is not immediately obvious why this works even should new peers be created while the deletion update is in progress.
In fact this is only possible by adding a small modification to the outlined process.
To show that the solution can work with the small modification we need to look at the two cases that can happen for a newly created peer, differentiated by where it first connects to.
If it connects to a peer that has already received the deletion update the solution would work by default without any modification.
This is the case since the new peer would receive the object model where the object has already been deleted.
It must thus not be notified of the deletion.
If, however, it connects to a peer that has yet to receive the deletion update even though it has already happened, we have a problem.
The new peer is not listed among the peers that must acknowledge the update even though it has the object.
Without the modification, this could result in the file being reintroduced by the new peer, undoing the deletion in effect.

Therefore peers must expand the first list of known peers if a single criteria is met.
This criteria is relatively straightforward: when receiving a deletion update each peer first checks the list against its own known peer list and in the case that they differ add all missing entries to the deletion update.
If a newly created peer has connected it will already be within the list and thus be ensured to have the deletion update hang around until it has received it.

Now a final note on what happens should a peer receive a deletion update for an object that it doesn't know.
In this case the message can not simply be silently discarded as there is no other representation of the update in the object model.
This could lead to orphaned updates if the peer acts as a bridge between two peers that were previously directly connected.
Therefore unknown deletion updates should be propagated if possible.
However, since orphaned updates can only happen as long as the peer list has not been fully updated between all peers, this is a sufficiently unlikely case that we can live with it.
By adding a time stamp we can even detect orphaned deletion updates and warn the user to help in solving the issue.
Alternatively we can also warn if we have too many possibly orphaned updates, although this would signify a larger issue.

\section{Update Detection and Reconciliation}
\label{sec:Update Detection and Reconciliation}

This section describes the theoretical side of the update detection: how we detect updates on file systems in a way that does not consume too much resources of the hosting computer.
Then we will discuss the interesting case of update reconciliation and how Tinzenite reacts to conflicts.

\subsection{Update Detection}
\label{sub:Update Detection}

Tinzenite continuously checks the directory against the model of the same (see also TODO).

TODO: I need to put some thought into avoiding a conflict between the disk watcher and when Tinzenite updates the objects... :P

\subsection{Update Reconciliation}
\label{sub:Update Reconciliation}

The core algorithm for how update reconciliation is done is another important element of Tinzenite.
The following section introduces it and discusses the ramifications for the system based on the initial model synchronization, leaving aside the per object update messages at first.
These will be discussed once the basic mode is understood.

The reconciliation begins by receiving the model of the other peer.
The model is temporarily stored so that Tinzenite can work with it until the reconciliation has finished.
To guarantee that the directory remains consistent with the model right from the start we propose a simple rule: the model of the local directory is updated atomically with the successful reception of the binary object updates.

For each object in the received and temporarily stored model Tinzenite performs a series of checks.
First and foremost if the object is listed in the deleted list it is ignored.

%TODO
TODO: how are conflicts handled.
Easiest: clients detect files are same but different, create the two as new versions and rename so user can see.
Nothing else needs to change for the API then â€“ if the user resolves it manually the rename / delete of files will propagate as normal.

Can I simplify the object model by using a dirty flag instead of versions?
I think so if both detect the conflict... but when can I reset the dirty flag?
This doesn't seem practical.
I'd have to store each other peer's state and send only the appropriate flags for this to work.

Using version numbers with the content hash etc should be easier.
If the version is the same the rest must be the same too, otherwise it is a conflict.
If higher version detected you can just overwrite it locally.

%TODO
TODO: on receiving an update: queue in connection specific command queue (one per connection because only one OP per connection) (update / redo existing fetch if applicable != FIFO); fetch first file in queue.
Then wait a bit (?) before propagating update yourself (decouple receive and send time frames?). <-- Isn't this implementation details?

\section{Advanced Features}
\label{sec:Advanced Features}

TODO: Here is the list of what I consider to be the advanced features.
Now do something sensible with it.

\begin{description}[leftmargin=2em,style=nextline,noitemsep,nolistsep]
\item[Encryption Key Management]
    An important capability will be the synchronization of the access keys to unencrypted clients so that all of them can access the data stored on encrypted clients.
    Thoughts should be given to how to revoke compromised keys.
\item[Space Management]
    The API must respect differences in storage size capabilities between different clients or size restrictions.
    Notably this is important for third parties to sell different storage sizes to clients.
    TODO: could files above the limit be sent as shadow files?
\item[Shadow Files]
    Depending on the location of a client a user may with to only access specific files without having to get an entire set or updates.
    Therefore the client could be set to only read and create dummy files.
    By selecting specific files the client will then only update or retrieve the corresponding files or directories.
    TODO: what to do if a client knows of shadow files but can't get full set?
    Are shadow files also transitively synchronized?
\end{description}
